{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9151118",
   "metadata": {},
   "source": [
    "# Algorithm Visualization and Performance Analysis\n",
    "\n",
    "This notebook loads trained sign language recognition models, evaluates their performance on the validation dataset, \n",
    "and visualizes metrics such as accuracy, loss, and class-wise confusion matrices.\n",
    "\n",
    "It also serves as the visualization and interpretability module for **Phase 1** of the Sign-to-Speech project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7baa4a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force Auto reload for imports\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b6efa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialized; reproducibility set.\n",
      "CUDA available: True\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti SUPER\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add src folder to path\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"Environment initialized; reproducibility set.\")\n",
    "\n",
    "# Device check\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if device == \"cuda\":\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(f\"Using GPU: {gpu_name}\")\n",
    "else:\n",
    "    try:\n",
    "        cpu_info = subprocess.check_output(\n",
    "            \"lscpu | grep 'Model name'\", shell=True\n",
    "        ).decode().strip().split(\":\")[1].strip()\n",
    "    except Exception:\n",
    "        cpu_info = \"Unknown CPU\"\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(f\"Using CPU: {cpu_info}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3da4e12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset paths resolved:\n",
      "   Project root     : /home/genesis/sem1_project/sign-to-speech-with-temporal-correction\n",
      "   Train directory  : /home/genesis/sem1_project/sign-to-speech-with-temporal-correction/data/labeled/train\n",
      "   Validation dir   : /home/genesis/sem1_project/sign-to-speech-with-temporal-correction/data/labeled/val\n",
      "\n",
      "Loaded 30 classes.\n",
      "Train samples: 78328, Validation samples: 8728\n",
      "Classes: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space', 'val']\n",
      "Loaded 30 classes.\n"
     ]
    }
   ],
   "source": [
    "# Imports from your project\n",
    "from src.dataset_loader import get_dataloaders\n",
    "from src.model_definitions import make_resnet18, make_mobilenet, make_cnn_lstm\n",
    "from src.train_labeled import train_model\n",
    "\n",
    "# Load dataset\n",
    "train_loader, val_loader, class_names = get_dataloaders(batch_size=32)\n",
    "print(f\"Loaded {len(class_names)} classes.\")\n",
    "\n",
    "# Choose model \n",
    "model_choice = \"resnet18\"   # change to \"mobilenet\" or \"cnn_lstm\" as needed\n",
    "num_classes = len(class_names)\n",
    "\n",
    "if model_choice == \"resnet18\":\n",
    "    model = make_resnet18(num_classes)\n",
    "elif model_choice == \"mobilenet\":\n",
    "    model = make_mobilenet(num_classes)\n",
    "else:\n",
    "    model = make_cnn_lstm(num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e73d4dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training script with default parameters...\n",
      "\n",
      "CUDA available: True\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti SUPER\n",
      " Dataset paths resolved:\n",
      "   Project root     : /home/genesis/sem1_project/sign-to-speech-with-temporal-correction\n",
      "   Train directory  : /home/genesis/sem1_project/sign-to-speech-with-temporal-correction/data/labeled/train\n",
      "   Validation dir   : /home/genesis/sem1_project/sign-to-speech-with-temporal-correction/data/labeled/val\n",
      "\n",
      "Loaded 30 classes.\n",
      "Train samples: 78328, Validation samples: 8728\n",
      "Classes: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space', 'val']\n",
      "\n",
      "Model Selected: resnet\n",
      "Training for up to 2 epochs (early stop patience = 30)\n",
      "\n",
      "\n",
      "Epoch 1/2:   0%|          | 0/2448 [00:00<?, ?batch/s]\n",
      "Epoch 1/2:   0%|          | 1/2448 [00:04<2:48:59,  4.14s/batch]\n",
      "Epoch 1/2:   0%|          | 2/2448 [00:04<1:12:40,  1.78s/batch]\n",
      "Epoch 1/2:   0%|          | 4/2448 [00:04<29:22,  1.39batch/s]  \n",
      "Epoch 1/2:   0%|          | 6/2448 [00:04<16:52,  2.41batch/s]\n",
      "Epoch 1/2:   0%|          | 8/2448 [00:04<11:31,  3.53batch/s]\n",
      "Epoch 1/2:   0%|          | 11/2448 [00:04<06:58,  5.82batch/s]\n",
      "Epoch 1/2:   1%|          | 14/2448 [00:05<04:46,  8.49batch/s]\n",
      "Epoch 1/2:   1%|          | 17/2448 [00:05<03:34, 11.35batch/s]\n",
      "Epoch 1/2:   1%|          | 20/2448 [00:05<02:50, 14.22batch/s]\n",
      "Epoch 1/2:   1%|          | 23/2448 [00:05<02:23, 16.90batch/s]\n",
      "Epoch 1/2:   1%|          | 26/2448 [00:05<02:05, 19.34batch/s]\n",
      "Epoch 1/2:   1%|          | 29/2448 [00:05<01:53, 21.38batch/s]\n",
      "Epoch 1/2:   1%|▏         | 32/2448 [00:05<01:45, 22.96batch/s]\n",
      "Epoch 1/2:   1%|▏         | 35/2448 [00:05<01:39, 24.29batch/s]\n",
      "Epoch 1/2:   2%|▏         | 38/2448 [00:05<01:35, 25.26batch/s]\n",
      "Epoch 1/2:   2%|▏         | 41/2448 [00:05<01:32, 25.94batch/s]\n",
      "Epoch 1/2:   2%|▏         | 44/2448 [00:06<01:30, 26.54batch/s]\n",
      "Epoch 1/2:   2%|▏         | 47/2448 [00:06<01:29, 26.95batch/s]\n",
      "Epoch 1/2:   2%|▏         | 50/2448 [00:06<01:28, 27.08batch/s]\n",
      "Epoch 1/2:   2%|▏         | 53/2448 [00:06<01:27, 27.37batch/s]\n",
      "Epoch 1/2:   2%|▏         | 56/2448 [00:06<01:26, 27.57batch/s]\n",
      "Epoch 1/2:   2%|▏         | 59/2448 [00:06<01:26, 27.52batch/s]\n",
      "Epoch 1/2:   3%|▎         | 62/2448 [00:06<01:27, 27.32batch/s]\n",
      "Epoch 1/2:   3%|▎         | 65/2448 [00:06<01:27, 27.38batch/s]\n",
      "Epoch 1/2:   3%|▎         | 68/2448 [00:06<01:26, 27.56batch/s]\n",
      "Epoch 1/2:   3%|▎         | 71/2448 [00:07<01:25, 27.72batch/s]\n",
      "Epoch 1/2:   3%|▎         | 74/2448 [00:07<01:25, 27.70batch/s]\n",
      "Epoch 1/2:   3%|▎         | 77/2448 [00:07<01:25, 27.78batch/s]\n",
      "Epoch 1/2:   3%|▎         | 80/2448 [00:07<01:24, 27.89batch/s]\n",
      "Epoch 1/2:   3%|▎         | 83/2448 [00:07<01:24, 27.88batch/s]\n",
      "Epoch 1/2:   4%|▎         | 86/2448 [00:07<01:24, 27.81batch/s]\n",
      "Epoch 1/2:   4%|▎         | 89/2448 [00:07<01:24, 27.91batch/s]\n",
      "Epoch 1/2:   4%|▍         | 92/2448 [00:07<01:24, 27.97batch/s]\n",
      "Epoch 1/2:   4%|▍         | 95/2448 [00:07<01:24, 27.84batch/s]\n",
      "Epoch 1/2:   4%|▍         | 98/2448 [00:08<01:24, 27.92batch/s]\n",
      "Epoch 1/2:   4%|▍         | 101/2448 [00:08<01:24, 27.93batch/s]\n",
      "Epoch 1/2:   4%|▍         | 104/2448 [00:08<01:24, 27.82batch/s]\n",
      "Epoch 1/2:   4%|▍         | 107/2448 [00:08<01:24, 27.86batch/s]\n",
      "Epoch 1/2:   4%|▍         | 110/2448 [00:08<01:23, 27.93batch/s]\n",
      "Epoch 1/2:   5%|▍         | 113/2448 [00:08<01:23, 27.84batch/s]\n",
      "Epoch 1/2:   5%|▍         | 116/2448 [00:08<01:23, 27.77batch/s]\n",
      "Epoch 1/2:   5%|▍         | 119/2448 [00:08<01:23, 27.82batch/s]\n",
      "Epoch 1/2:   5%|▍         | 122/2448 [00:08<01:23, 27.74batch/s]\n",
      "Epoch 1/2:   5%|▌         | 125/2448 [00:09<01:23, 27.76batch/s]\n",
      "Epoch 1/2:   5%|▌         | 128/2448 [00:09<01:23, 27.79batch/s]\n",
      "Epoch 1/2:   5%|▌         | 131/2448 [00:09<01:23, 27.68batch/s]\n",
      "Epoch 1/2:   5%|▌         | 134/2448 [00:09<01:23, 27.68batch/s]\n",
      "Epoch 1/2:   6%|▌         | 137/2448 [00:09<01:23, 27.83batch/s]\n",
      "Epoch 1/2:   6%|▌         | 140/2448 [00:09<01:23, 27.79batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning training script with default parameters...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m process = subprocess.Popen(\n\u001b[32m      7\u001b[39m     [\u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m../src/train_labeled.py\u001b[39m\u001b[33m\"\u001b[39m],   \u001b[38;5;66;03m# <-- notice the ../ here\u001b[39;00m\n\u001b[32m      8\u001b[39m     stdout=subprocess.PIPE,\n\u001b[32m      9\u001b[39m     stderr=subprocess.STDOUT,\n\u001b[32m     10\u001b[39m     text=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     11\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m process.wait()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "import subprocess\n",
    "\n",
    "print(\"Running training script with default parameters...\\n\")\n",
    "\n",
    "process = subprocess.Popen(\n",
    "    [\"python\", \"../src/train_labeled.py\"],   # <-- notice the ../ here\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "for line in process.stdout:\n",
    "    print(line, end=\"\")\n",
    "\n",
    "process.wait()\n",
    "print(\"\\nTraining script finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5d1f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization from training_log.csv ---\n",
    "\n",
    "\n",
    "log_path = \"../results/training_log.csv\"\n",
    "if os.path.exists(log_path):\n",
    "    df = pd.read_csv(log_path)\n",
    "    print(df.tail())\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(df[\"epoch\"], df[\"train_acc\"], label=\"Train Accuracy\", linewidth=2)\n",
    "    plt.plot(df[\"epoch\"], df[\"val_acc\"], label=\"Validation Accuracy\", linewidth=2)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"Training vs Validation Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(df[\"epoch\"], df[\"loss\"], label=\"Training Loss\", color=\"red\", linewidth=2)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training log found yet. Train the model first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a91e7e",
   "metadata": {},
   "source": [
    "## Summary of Evaluation\n",
    "\n",
    "- Loaded the trained model checkpoint\n",
    "- Evaluated accuracy and generated classification report\n",
    "- Visualized confusion matrix and sample predictions\n",
    "- (Optional) Displayed training curves from logs\n",
    "\n",
    "Next step: integrate real-time temporal correction visualization once the Phase 2 module is ready.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
